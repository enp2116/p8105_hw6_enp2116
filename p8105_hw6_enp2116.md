p8105_hw6_enp2116
================
Emily Potts
2022-11-30

# Problem 1

``` r
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  select(name, id, everything())
```

    ## Registered S3 method overwritten by 'hoardr':
    ##   method           from
    ##   print.cache_info httr

    ## using cached file: ~/Library/Caches/R/noaa_ghcnd/USW00094728.dly

    ## date created (size, mb): 2022-09-29 10:31:53 (8.401)

    ## file min/max dates: 1869-01-01 / 2022-09-30

First, we will get a distribution for $\hat{r^2}$ by drawing 5000
bootstrap samples; adding SLR models (with tmax as the response and tmin
as the predictor) to each sample. Then, we extract the $\hat{r^2}$ value
from the model results and plot the density.

``` r
r_squared_estimates =
  weather_df %>% 
  modelr::bootstrap(n = 5000) %>% 
  mutate(
    lin_models = map(strap, ~lm(tmax ~ tmin, data = .x) ),
    lin_mod_results = map(lin_models, broom::glance)
    ) %>% 
  select(-strap, -lin_models) %>% 
  unnest(lin_mod_results)

r_squared_estimates %>% 
  ggplot(aes(x = r.squared)) + geom_density() + labs(
    title = "Distribution of R Squared",
    x = "R Squared",
    y = "Density")
```

![](p8105_hw6_enp2116_files/figure-gfm/unnamed-chunk-2-1.png)<!-- -->

``` r
quantile(r_squared_estimates$r.squared, na.rm = T,probs = c(0.025, 0.975))
```

    ##      2.5%     97.5% 
    ## 0.8941280 0.9276839

In this plot, we can see that the $\hat{r}^2$ value is high (peak at
around 0.91) and that the distribution has a left-skew (most likely due
to upper bound at 1). The 2.5% and 97.5% quantiles of the estimates
across our bootstrap samples allow to construct a 95% CI for ${r}^2$,
which is (0.894, 0.928). Since our distribution is not symmetric, this
is a better approximation than the standard CI formula (mean +/-
1.96\*SE).

Next, we will get a distribution for $log(\hat{\beta_0 * \beta_1})$ by a
similar process: drawing bootstrap samples; adding SLR models (with tmax
as the response and tmin as the predictor) to each sample and extracting
the $log(\hat{\beta_0 * \beta_1})$ value to plot the density.

``` r
log_beta_product_est =
weather_df %>% 
  modelr::bootstrap(n = 5000) %>% 
  mutate(
    lin_models = map(strap, ~lm(tmax ~ tmin, data = .x) ),
    lin_mod_results = map(lin_models, broom::tidy)) %>% 
  select(-strap, -lin_models) %>% 
  unnest(lin_mod_results) %>% 
  select(id = `.id`, term, estimate) %>% 
  pivot_wider(
    names_from = term, 
    values_from = estimate) %>% 
  rename(beta0 = `(Intercept)`, beta1 = tmin) %>% 
  mutate(log_b0b1 = log(beta0 * beta1)) 

log_beta_product_est %>% 
  ggplot(aes(x = log_b0b1)) + geom_density() + labs(
    title = "Distribution of Log(Beta 0 * Beta 1)",
    x = "Log(Beta 0 * Beta 1)",
    y = "Density")
```

![](p8105_hw6_enp2116_files/figure-gfm/unnamed-chunk-3-1.png)<!-- -->

``` r
quantile(log_beta_product_est$log_b0b1, na.rm = T,probs = c(0.025, 0.975))
```

    ##     2.5%    97.5% 
    ## 1.964571 2.058591

From the plot, we can see that the $log(\hat{\beta_0 * \beta_1})$
distribution is also somewhat left-skewed (outliers with a lower
$log(\hat{\beta_0 * \beta_1})$) but has a peak at around 2.01. The 2.5%
and 97.5% quantiles of the estimates across bootstrap samples allow us
to construct a 95% CI for $log(\hat{\beta_0 * \beta_1})$, which is
(1.965, 2.060). Again, since our distribution is not symmetric, this is
a better approximation than the standard CI formula (mean +/- 1.96\*SE).

# Problem 2

Washington Post homicides:

Creating a city_state variable and a binary variable (outcome)
indicating whether the homicide is solved. Omitting cities Dallas, TX;
Phoenix, AZ; Kansas City; and MO Tulsa, AL. Limiting analysis those for
whom victim_race is white or black, and making sure that victim_age is
numeric.

``` r
tidy_homicide = 
  read_csv("data/homicide-data.csv") %>% 
  mutate(
    city_state = paste(city, state, sep = ", "),
    resolved = as.numeric(disposition == "Closed by arrest"),
    victim_age = as.numeric(victim_age)) %>% 
  filter(!city_state %in% c("Dallas, TX", "Phoenix, AZ", "Kansas City, MO", "Tulsa, AL")) %>% 
  filter(victim_race  %in% c( "Black", "White"))
```

    ## Rows: 52179 Columns: 12
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (9): uid, victim_last, victim_first, victim_race, victim_age, victim_sex...
    ## dbl (3): reported_date, lat, lon
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

For the city of Baltimore, MD, using the glm function to fit a logistic
regression with resolved vs unresolved as the outcome and victim age,
sex and race as predictors. Saving the output of glm as an R object;
applying the broom::tidy to this object; and obtaining the estimate and
confidence interval of the adjusted odds ratio for solving homicides
comparing male victims to female victims keeping all other variables
fixed:

``` r
balt_log_fit = 
  tidy_homicide %>% 
  filter(city_state == "Baltimore, MD") %>% 
  glm(resolved ~ victim_age + victim_race + victim_sex, data = ., family = binomial()) 

balt_log_fit %>% 
  broom::tidy() %>% 
  mutate(OR = exp(estimate),
         est_low_CI = estimate - std.error * qnorm(0.975),
         est_high_CI = estimate + std.error * qnorm(0.975),
         OR_low_CI = exp(est_low_CI),
         OR_high_CI = exp(est_high_CI),
         ) %>% 
  filter(term == "victim_sexMale") %>% 
  select(term, OR, OR_low_CI, OR_high_CI, p.value) %>% 
  knitr::kable(digits = 3)
```

| term           |    OR | OR_low_CI | OR_high_CI | p.value |
|:---------------|------:|----------:|-----------:|--------:|
| victim_sexMale | 0.426 |     0.325 |      0.558 |       0 |

We are 95% confident that the adjusted odds ratio for solving homicides
comparing male victims to female victims keeping victim race and age
fixed is between 0.325 and 0.558. Since this value is less than one,
this means that when controlling for the impacts of other victim-related
variables, homicides in which the victims are male are less likely to be
resolved that those in which the victim is female.

Now running glm for each of the cities in my dataset, and extracting the
adjusted odds ratio (and CI) for solving homicides comparing male
victims to female victims. Do this within a “tidy” pipeline, making use
of purrr::map, list columns, and unnest as necessary to create a
dataframe with estimated ORs and CIs for each city.

``` r
city_state_glms =
  tidy_homicide %>% 
  nest(data = -city_state) %>% 
  mutate(
    glms = map(data, ~glm(resolved ~ victim_age + victim_race + victim_sex, data = ., family = binomial())),
    results = map(glms, broom::tidy)) %>% 
  select(-data, -glms) %>% 
  unnest(results) %>% 
  filter(term == "victim_sexMale") %>% 
  mutate(OR = exp(estimate),
         est_low_CI = estimate - std.error * qnorm(0.975),
         est_high_CI = estimate + std.error * qnorm(0.975),
         OR_low_CI = exp(est_low_CI),
         OR_high_CI = exp(est_high_CI),
         ) %>% 
  select(city_state, term, OR, OR_low_CI, OR_high_CI, p.value) 
head(city_state_glms)
```

    ## # A tibble: 6 × 6
    ##   city_state      term              OR OR_low_CI OR_high_CI  p.value
    ##   <chr>           <chr>          <dbl>     <dbl>      <dbl>    <dbl>
    ## 1 Albuquerque, NM victim_sexMale 1.77      0.831      3.76  1.39e- 1
    ## 2 Atlanta, GA     victim_sexMale 1.00      0.684      1.46  1.00e+ 0
    ## 3 Baltimore, MD   victim_sexMale 0.426     0.325      0.558 6.26e-10
    ## 4 Baton Rouge, LA victim_sexMale 0.381     0.209      0.695 1.65e- 3
    ## 5 Birmingham, AL  victim_sexMale 0.870     0.574      1.32  5.11e- 1
    ## 6 Boston, MA      victim_sexMale 0.674     0.356      1.28  2.26e- 1

Create a plot that shows the estimated ORs and CIs for each city.
Organize cities according to estimated OR, and comment on the plot.

``` r
city_state_glms %>% 
  mutate(city_state = forcats::fct_reorder(city_state, OR)) %>% 
  ggplot(aes(x = city_state, y = OR, color = city_state)) + geom_point() + geom_errorbar(aes(ymin = OR_low_CI, ymax = OR_high_CI)) +
  theme(axis.text.x = element_text(angle = 80, hjust = 1, size = 7))  + 
  labs(
    title = "Adjusted Odds Ratios for Male to Female Solved Homicides",
    x = "Location (City, State)",
    y = "Adjusted OR and 95% CI"
    ) +  
  theme(legend.position = "None")
```

![](p8105_hw6_enp2116_files/figure-gfm/unnamed-chunk-7-1.png)<!-- -->
Based on the plot, we can see that the majority of cities have an
adjusted odds ratio less than 1, indicating that when controlling for
the impacts of other victim-related variables, homicides in which the
victims are male are less likely to be resolved that those in which the
victim is female. We do however, see some cities with quite wide 95% CIs
for this odds ratio. This occurs primarily in cities that have the
highest male to female adjusted ORs (the right side of the graph).

# Problem 3

Loading and clean the data for regression analysis (i.e. convert numeric
to factor where appropriate, check for missing data, etc.).

``` r
tidy_birthweight =
  read_csv("data/birthweight.csv", col_types = "fnnnnnfnfnnnfnnnnnnnn")

tidy_birthweight %>% summarise_all(list(name = ~sum(is.na(.))/length(.)))
```

    ## # A tibble: 1 × 20
    ##   babysex_name bhead_n…¹ bleng…² bwt_n…³ delwt…⁴ finco…⁵ frace…⁶ gawee…⁷ malfo…⁸
    ##          <dbl>     <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>
    ## 1            0         0       0       0       0       0       0       0       0
    ## # … with 11 more variables: menarche_name <dbl>, mheight_name <dbl>,
    ## #   momage_name <dbl>, mrace_name <dbl>, parity_name <dbl>, pnumlbw_name <dbl>,
    ## #   pnumsga_name <dbl>, ppbmi_name <dbl>, ppwt_name <dbl>, smoken_name <dbl>,
    ## #   wtgain_name <dbl>, and abbreviated variable names ¹​bhead_name,
    ## #   ²​blength_name, ³​bwt_name, ⁴​delwt_name, ⁵​fincome_name, ⁶​frace_name,
    ## #   ⁷​gaweeks_name, ⁸​malform_name

Propose a regression model for birthweight. This model may be based on a
hypothesized structure for the factors that underly birthweight, on a
data-driven model-building process, or a combination of the two.

``` r
bw_reg_model = 
  tidy_birthweight %>% 
  lm(data = ., bwt ~ blength + delwt*ppwt + gaweeks + fincome + mrace + parity  + smoken) %>%
  summary()
```

I built my proposed model using backward elimination, initially starting
with the variables of blength, delwt, gaweeks, fincome , malform,
momage, mrace, parity, ppwt, and smoken. I chose these variables based
on recent literature indicating key baby/pregnancy measures and
mother-centric (both demographic and situational) measures that have
been linked to a baby’s low birthweight. I first removed malform, as it
had the highest alpha of 0.717. After running the regression again, I
removed momage, as it had the next highest alpha of 0.375.

Describe your modeling process and show a plot of model residuals
against fitted values – use add_predictions and add_residuals in making
this plot.

Compare your model to two others:

One using length at birth and gestational age as predictors (main
effects only)

``` r
main_effects_mod = 
  tidy_birthweight %>% 
  lm(data = ., bwt ~ blength + gaweeks) 
```

One using head circumference, length, sex, and all interactions
(including the three-way interaction) between these

``` r
interaction_mod = 
  tidy_birthweight %>% 
  lm(data = ., bwt ~ bhead*blength*babysex)
```

Make this comparison in terms of the cross-validated prediction error;
use crossv_mc and functions in purrr as appropriate.

``` r
cv_df =
  crossv_mc(tidy_birthweight, 100) %>% 
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble))
```

``` r
cv_df = 
  cv_df %>% 
  mutate(
    bw_reg_model  = map(train, ~lm(data = ., bwt ~ blength + delwt + gaweeks + fincome + mrace + parity + ppwt + smoken)),
    main_effects_mod   = map(train, ~lm(data = ., bwt ~ blength + gaweeks)),
    interaction_mod  = map(train, ~lm(data = ., bwt ~ bhead*blength*babysex))) %>% 
  mutate(
    rmse_bw_reg = map2_dbl(bw_reg_model, test, ~rmse(model = .x, data = .y)),
    rmse_main_effects  = map2_dbl(main_effects_mod, test, ~rmse(model = .x, data = .y)),
    rmse_interaction = map2_dbl(interaction_mod, test, ~rmse(model = .x, data = .y)))
```

    ## Warning in predict.lm(model, data): prediction from a rank-deficient fit may be
    ## misleading

Violin plot showing the prediction error distribution for each candidate
model:

``` r
cv_df %>% 
  select(starts_with("rmse")) %>% 
  pivot_longer(
    everything(),
    names_to = "model", 
    values_to = "rmse",
    names_prefix = "rmse_") %>% 
  mutate(model = fct_inorder(model)) %>% 
  ggplot(aes(x = model, y = rmse)) + geom_violin()
```

![](p8105_hw6_enp2116_files/figure-gfm/unnamed-chunk-15-1.png)<!-- -->
